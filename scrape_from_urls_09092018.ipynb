{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "from itertools import product\n",
    "from urllib.request import Request\n",
    "import urllib\n",
    "import sys\n",
    "import json\n",
    "import PyPDF2\n",
    "import collections\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import math\n",
    "import string\n",
    "import textmining as txtm\n",
    "\n",
    "\n",
    "#working directory path\n",
    "path = 'C:/Users/Lenovo/Desktop/NLP/'\n",
    "#path = 'C:/Users/user/Python/Natural Language Processsing/'\n",
    "os.chdir(path)\n",
    "\n",
    "def download_file(download_url,fname='document.pdf'):\n",
    "    if sys.version_info[0]==2:\n",
    "        response = urllib.urlopen(download_url) # python 2\n",
    "    else:\n",
    "        response = urllib.request.urlopen(download_url) # python 3\n",
    "\n",
    "    file = open(fname, 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    "    \n",
    "#check if the html files contain the word 'privacy' \n",
    "#restrict documents that have length less than 100 words\n",
    "def check_privacy(firm,url,text,min_words=100,domain = ['.com','.gov']):\n",
    "    if domain[0] not in url:\n",
    "    #if (domain[0] not in url and domain[1] not in url): \n",
    "        flag = \"Error:  in url %s corresponding to %s No %s domain\" % (url,firm,domain)\n",
    "    elif ('privacy' not in text.lower()):\n",
    "        flag = \"Error:  in url %s corresponding to %s Does not contain the word privacy\" % (url , firm )\n",
    "    elif len(str(text).split()) < min_words:\n",
    "        flag = \"Error : in url %s corresponding to %s Less than %s words\" %  (url , firm ,str(min_words)) \n",
    "    else:\n",
    "        flag = \"url %s corresponding to %s is _OK_\" % (url,firm)\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords from R's 'tm' library\n",
    "# import state names of US\n",
    "\n",
    "rsw = pd.read_csv('C:/Users/Lenovo/Desktop/NLP/rstopwords.csv')['x'].values.tolist() \n",
    "\n",
    "states = pd.read_csv('C:/Users/Lenovo/Desktop/NLP/states.csv')['State'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load urls\n",
    "with open(\"C:/Users/Lenovo/Desktop/NLP/compustat_trial__ggl_urls.json\",'r') as fp:\n",
    "    urls = json.load(fp)\n",
    "   \n",
    "firms = list(urls.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(firms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = list(urls.keys())[0:105:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firm: STANDARD MOTOR PRODUCTS , URL 0, Downloaded 3149 chars\n",
      "Firm: STANDARD MOTOR PRODUCTS , URL 1, Downloaded 10865 chars\n",
      "Firm: STANDARD MOTOR PRODUCTS , URL 2, Downloaded 5220 chars\n",
      "Firm: STANDARD MOTOR PRODUCTS , URL 3, Error: HTTP Error 406: Not Acceptable\n",
      "Firm: STANDARD MOTOR PRODUCTS , URL 4, Downloaded 1514 chars\n",
      "Firm: STANDEX INTERNATIONAL , URL 0, Downloaded 17945 chars\n",
      "Firm: STANDEX INTERNATIONAL , URL 1, Downloaded 1819 chars\n",
      "Firm: STANDEX INTERNATIONAL , URL 2, Downloaded 35127 chars\n",
      "Firm: STANDEX INTERNATIONAL , URL 3, Downloaded 13102 chars\n",
      "Firm: STANDEX INTERNATIONAL , URL 4, Downloaded 49848 chars\n",
      "Firm: BAYER AG , URL 0, Error: 'NoneType' object has no attribute 'next_element'\n",
      "Firm: BAYER AG , URL 1, Downloaded 18024 chars\n",
      "Firm: BAYER AG , URL 2, Downloaded 13248 chars\n",
      "Firm: BAYER AG , URL 3, Downloaded 21718 chars\n",
      "Firm: BAYER AG , URL 4, Downloaded 3908 chars\n",
      "Firm: DEUTSCHE LUFTHANSA AG , URL 0, Error: HTTP Error 405: Method Not Allowed\n",
      "Firm: DEUTSCHE LUFTHANSA AG , URL 1, Downloaded 26085 chars\n",
      "Firm: DEUTSCHE LUFTHANSA AG , URL 2, Error: HTTP Error 405: Method Not Allowed\n",
      "Firm: DEUTSCHE LUFTHANSA AG , URL 3, Downloaded 12774 chars\n",
      "Firm: DEUTSCHE LUFTHANSA AG , URL 4, Error: HTTP Error 405: Method Not Allowed\n",
      "Firm: AMCOR , URL 0, Downloaded 0 chars\n",
      "Firm: AMCOR , URL 1, Downloaded 7855 chars\n",
      "Firm: AMCOR , URL 2, Downloaded 0 chars\n",
      "Firm: AMCOR , URL 3, Downloaded 7106 chars\n",
      "Firm: AMCOR , URL 4, Downloaded 0 chars\n",
      "Firm: STATE STREET , URL 0, Downloaded 384483 chars\n",
      "Firm: STATE STREET , URL 1, Error: [WinError 2] The system cannot find the file specified: 'PDF'\n",
      "Firm: STATE STREET , URL 2, Downloaded 354316 chars\n",
      "Firm: STATE STREET , URL 3, Downloaded 4884 chars\n",
      "Firm: STATE STREET , URL 4, Downloaded 371737 chars\n",
      "Firm: STEPAN , URL 0, Downloaded 12415 chars\n",
      "Firm: STEPAN , URL 1, Downloaded 24598 chars\n",
      "Firm: STEPAN , URL 2, Downloaded 8060 chars\n",
      "Firm: STEPAN , URL 3, Downloaded 11284 chars\n",
      "Firm: STEPAN , URL 4, Downloaded 13286 chars\n",
      "Firm: VOLKSWAGEN AG , URL 0, Downloaded 9011 chars\n",
      "Firm: VOLKSWAGEN AG , URL 1, Downloaded 10931 chars\n",
      "Firm: VOLKSWAGEN AG , URL 2, Downloaded 18093 chars\n",
      "Firm: VOLKSWAGEN AG , URL 3, Downloaded 6737 chars\n",
      "Firm: VOLKSWAGEN AG , URL 4, Downloaded 19049 chars\n",
      "Firm: NIELSEN HOLDINGS , URL 0, Downloaded 21447 chars\n",
      "Firm: NIELSEN HOLDINGS , URL 1, Downloaded 47413 chars\n",
      "Firm: NIELSEN HOLDINGS , URL 2, Downloaded 12556 chars\n",
      "Firm: NIELSEN HOLDINGS , URL 3, Downloaded 19240 chars\n",
      "Firm: NIELSEN HOLDINGS , URL 4, Downloaded 23010 chars\n",
      "Firm: PUBLIC STORAGE , URL 0, Downloaded 5147 chars\n",
      "Firm: PUBLIC STORAGE , URL 1, Downloaded 39930 chars\n",
      "Firm: PUBLIC STORAGE , URL 2, Downloaded 26219 chars\n",
      "Firm: PUBLIC STORAGE , URL 3, Error: HTTP Error 406: Not Acceptable\n",
      "Firm: PUBLIC STORAGE , URL 4, Downloaded 4317 chars\n",
      "Firm: STRYKER , URL 0, Downloaded 17442 chars\n",
      "Firm: STRYKER , URL 1, Downloaded 24727 chars\n",
      "Firm: STRYKER , URL 2, Error: HTTP Error 406: Not Acceptable\n",
      "Firm: STRYKER , URL 3, Downloaded 16127 chars\n",
      "Firm: STRYKER , URL 4, Downloaded 8799 chars\n",
      "Firm: NAVIENT , URL 0, Downloaded 2626 chars\n",
      "Firm: NAVIENT , URL 1, Downloaded 1032 chars\n",
      "Firm: NAVIENT , URL 2, Downloaded 7601 chars\n",
      "Firm: NAVIENT , URL 3, Downloaded 495 chars\n",
      "Firm: NAVIENT , URL 4, Downloaded 12846 chars\n",
      "Firm: STURM RUGER , URL 0, Downloaded 42094 chars\n",
      "Firm: STURM RUGER , URL 1, Downloaded 20941 chars\n",
      "Firm: STURM RUGER , URL 2, Downloaded 2295 chars\n",
      "Firm: STURM RUGER , URL 3, Downloaded 19155 chars\n",
      "Firm: STURM RUGER , URL 4, Downloaded 2168 chars\n",
      "Firm: NOVARTIS AG , URL 0, Downloaded 15292 chars\n",
      "Firm: NOVARTIS AG , URL 1, Downloaded 12788 chars\n",
      "Firm: NOVARTIS AG , URL 2, Downloaded 6381 chars\n",
      "Firm: NOVARTIS AG , URL 3, Downloaded 18692 chars\n",
      "Firm: NOVARTIS AG , URL 4, Downloaded 27850 chars\n",
      "Firm: SUNTRUST BANKS , URL 0, Downloaded 430112 chars\n",
      "Firm: SUNTRUST BANKS , URL 1, Downloaded 5602 chars\n",
      "Firm: SUNTRUST BANKS , URL 2, Downloaded 443910 chars\n",
      "Firm: SUNTRUST BANKS , URL 3, Downloaded 6046 chars\n",
      "Firm: SUNTRUST BANKS , URL 4, Downloaded 8219 chars\n",
      "Firm: SUPERIOR INDUSTRIES , URL 0, Downloaded 17036 chars\n",
      "Firm: SUPERIOR INDUSTRIES , URL 1, Downloaded 6793 chars\n",
      "Firm: SUPERIOR INDUSTRIES , URL 2, Error: HTTP Error 406: Not Acceptable\n",
      "Firm: SUPERIOR INDUSTRIES , URL 3, Downloaded 2988 chars\n",
      "Firm: SUPERIOR INDUSTRIES , URL 4, Downloaded 2399 chars\n",
      "Firm: SYNALLOY , URL 0, Downloaded 4309 chars\n",
      "Firm: SYNALLOY , URL 1, Downloaded 4737 chars\n",
      "Firm: SYNALLOY , URL 2, Downloaded 3103 chars\n",
      "Firm: SYNALLOY , URL 3, Downloaded 16028 chars\n",
      "Firm: SYNALLOY , URL 4, Downloaded 4159 chars\n",
      "Firm: SAPPI , URL 0, Error: [WinError 2] The system cannot find the file specified: 'PDF'\n",
      "Firm: SAPPI , URL 1, Downloaded 5603 chars\n",
      "Firm: SAPPI , URL 2, Downloaded 3163 chars\n",
      "Firm: SAPPI , URL 3, Downloaded 4188 chars\n",
      "Firm: SAPPI , URL 4, Downloaded 1368 chars\n",
      "Firm: SYSCO , URL 0, Downloaded 26564 chars\n",
      "Firm: SYSCO , URL 1, Downloaded 102 chars\n",
      "Firm: SYSCO , URL 2, Downloaded 13429 chars\n",
      "Firm: SYSCO , URL 3, Downloaded 4875 chars\n",
      "Firm: SYSCO , URL 4, Downloaded 108 chars\n",
      "Firm: CGG , URL 0, Downloaded 71807 chars\n",
      "Firm: CGG , URL 1, Downloaded 14979 chars\n",
      "Firm: CGG , URL 2, Downloaded 66891 chars\n",
      "Firm: CGG , URL 3, Downloaded 10044 chars\n",
      "Firm: CGG , URL 4, Downloaded 58008 chars\n",
      "Firm: SAP SE , URL 0, Downloaded 284758 chars\n",
      "Firm: SAP SE , URL 1, Downloaded 66393 chars\n",
      "Firm: SAP SE , URL 2, Downloaded 31105 chars\n",
      "Firm: SAP SE , URL 3, Downloaded 37115 chars\n",
      "Firm: SAP SE , URL 4, Downloaded 0 chars\n",
      "Firm: TEJON RANCH , URL 0, Downloaded 4727 chars\n",
      "Firm: TEJON RANCH , URL 1, Downloaded 6507 chars\n",
      "Firm: TEJON RANCH , URL 2, Downloaded 1603 chars\n",
      "Firm: TEJON RANCH , URL 3, Downloaded 14277 chars\n",
      "Firm: TEJON RANCH , URL 4, Downloaded 1765 chars\n",
      "Firm: TELEFLEX , URL 0, Downloaded 37072 chars\n",
      "Firm: TELEFLEX , URL 1, Downloaded 0 chars\n",
      "Firm: TELEFLEX , URL 2, Error: [WinError 2] The system cannot find the file specified: 'PDF'\n",
      "Firm: TELEFLEX , URL 3, Downloaded 11441 chars\n",
      "Firm: TELEFLEX , URL 4, Downloaded 1292 chars\n",
      "Firm: TENNECO , URL 0, Downloaded 25556 chars\n",
      "Firm: TENNECO , URL 1, Downloaded 0 chars\n",
      "Firm: TENNECO , URL 2, Downloaded 14126 chars\n",
      "Firm: TENNECO , URL 3, Downloaded 2660 chars\n",
      "Firm: TENNECO , URL 4, Downloaded 109503 chars\n",
      "Firm: TENNESSEE VALLEY AUTHORITY , URL 0, Downloaded 5053 chars\n",
      "Firm: TENNESSEE VALLEY AUTHORITY , URL 1, Downloaded 437184 chars\n",
      "Firm: TENNESSEE VALLEY AUTHORITY , URL 2, Downloaded 14396 chars\n",
      "Firm: TENNESSEE VALLEY AUTHORITY , URL 3, Downloaded 438791 chars\n",
      "Firm: TENNESSEE VALLEY AUTHORITY , URL 4, Downloaded 6285 chars\n",
      "Firm: AMERICAN AIRLINES GROUP , URL 0, Downloaded 10814 chars\n",
      "Firm: AMERICAN AIRLINES GROUP , URL 1, Downloaded 0 chars\n",
      "Firm: AMERICAN AIRLINES GROUP , URL 2, Downloaded 92175 chars\n",
      "Firm: AMERICAN AIRLINES GROUP , URL 3, Error: [WinError 2] The system cannot find the file specified: 'PDF'\n",
      "Firm: AMERICAN AIRLINES GROUP , URL 4, Downloaded 20164 chars\n",
      "Firm: TERADYNE , URL 0, Downloaded 7793 chars\n",
      "Firm: TERADYNE , URL 1, Downloaded 2577 chars\n",
      "Firm: TERADYNE , URL 2, Downloaded 4880 chars\n",
      "Firm: TERADYNE , URL 3, Downloaded 1327 chars\n",
      "Firm: TERADYNE , URL 4, Downloaded 4395 chars\n"
     ]
    }
   ],
   "source": [
    "# create master dictionary \n",
    "scrape_policies = {}\n",
    "scrape_errors = {}\n",
    "replace_strings = [\"\\\\xcb\\\\x9\",\"\\\\xc5\\\\x\",\"\\\\xef\\\\xac\\\\x81\",\"\\\\xef\\\\xac\\\\x82\",\"\\b\",\n",
    "                   \"\\\\xe2\\\\x84\\\\xa2s\",\"\\\\\\\\xe2\\\\\\\\x84\\\\\\\\xa2s\",\"\\\\', b\\\\'\",\n",
    "                   \"\\\\\\\\xc5\\\\\\\\xa0\",\"\\\\\\\\xe2\\\\\\\\x84\\\\\\\\xa2\",\"\\\\xe2\\\\x84\\\\xa2\"]\n",
    "\n",
    "\n",
    "# access the text extracted for each firm using indexes\n",
    "# change in Load_urls tab from the next line to run for all firms.\n",
    "for firm in firms:\n",
    "    scrape_policies[firm] = {}\n",
    "    scrape_errors[firm] = {}\n",
    "\n",
    "    for index in [0,1,2,3,4]:\n",
    "        policies = \"\"\n",
    "        length = 0\n",
    "        try:\n",
    "            path = 'C:/Users/Lenovo/Desktop/NLP/'\n",
    "            #path = 'C:/Users/user/Python/Natural Language Processsing/'\n",
    "            os.chdir(path)\n",
    "            if urls[firm][index].endswith('.pdf'):\n",
    "                os.chdir('PDF')\n",
    "                #check if the file is pdf\n",
    "                fname=\"%s_%s.pdf\" % (firm,str(index))\n",
    "                download_file(urls[firm][index], fname)\n",
    "                \n",
    "                #extract texts from the pdfs\n",
    "                pdf_file = open(fname, 'rb')\n",
    "                read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "                number_of_pages = read_pdf.getNumPages()\n",
    "                c = collections.Counter(range(number_of_pages))\n",
    "                content = \"\"\n",
    "                for i in c:\n",
    "                    page = read_pdf.getPage(i)\n",
    "                    page_content = page.extractText()\n",
    "                    content = content + ' ' + str(page_content.encode('utf-8'))\n",
    "                for string in replace_strings:\n",
    "                    content = content.replace(string,'')   \n",
    "                \n",
    "                \n",
    "                flag = 'Downloaded %s chars from %s document' % (len(str(content)),fname)\n",
    "                policies = content\n",
    "            else:\n",
    "                sauce = urllib.request.urlopen(Request(urls[firm][index], headers={'User-Agent': 'Mozilla/5.0'})).read()\n",
    "                soup = bs.BeautifulSoup(sauce,'html5lib')\n",
    "                for i in list(range(1,len(soup.find_all(['p','ul','table'])))):\n",
    "                    text = soup.find_all(['p','ul','table'])[i].text\n",
    "                    policies = policies + ' ' + text\n",
    "                    length= length+len(text)\n",
    "                flag = \"Downloaded %d chars\" % length\n",
    "            \n",
    "        except Exception as e:\n",
    "            flag = \"Error: \" + str(e)\n",
    "            policies = flag\n",
    "        finally:\n",
    "            print('Firm: %s, URL %s, %s' % (firm,str(index),flag))\n",
    "        if len(policies) != 0:\n",
    "            scrape_policies[firm][index] = policies\n",
    "        else:\n",
    "            #'a an the' has been used only for indexing purpose - will any way be removed for not meeting 100 words criteria\n",
    "            scrape_policies[firm][index] = 'a an the'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove policies that do not contain 100 words (minimum length)\n",
    "# Also checking whether the text contains 'privacy' or not using the check_privacy function.\n",
    " \n",
    "okurl = {}\n",
    "for i in firms:\n",
    "    okurl[i] = {}\n",
    "\n",
    "for firm in firms:\n",
    "    position = []\n",
    "    for index in range(0,5):\n",
    "        temp = (check_privacy(firm = firm , url = urls[firm][index] , text = scrape_policies[firm][index]))\n",
    "        if '_OK_' in temp:\n",
    "            position.append(index)\n",
    "            print('%s index %s' % (temp , index) )\n",
    "        okurl[firm] = position\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare texts extracted from both html files and pdfs\n",
    "#select the one that has the longest length from the list (pdf & html included)\n",
    "def testFuncNew(text= 'This is the sample text.',sw=rsw):\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in sw])\n",
    "    return text\n",
    "\n",
    "for firm in firms:\n",
    "    length = []\n",
    "    if len(okurl[firm]) != 0:\n",
    "        #for i in okurl[firm]:\n",
    "            #length.append(len(testFuncNew(scrape_policies[firm][i])))\n",
    "\n",
    "        #maxvalue = max(length)\n",
    "        okurl[firm] = [okurl[firm][0]]\n",
    "    if len(okurl[firm]) == 0:\n",
    "        del okurl[firm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = list(okurl.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to be used to extract headers \n",
    "def testFuncNew(text= 'This is sample text.',sw=rsw,key = 'e'):\n",
    "    if key == 'e':\n",
    "        text = text.lower()\n",
    "        text = ' '.join([word for word in text.split() if word not in sw])\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        text = ' '.join([word for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and store header of paragraphs\n",
    "# Dictionary for storing and accessing headers.\n",
    "\n",
    "Header_policies = []\n",
    "Paragraph_policies= []\n",
    "Headers_Lengths = []\n",
    "\n",
    "#function to be used to extract headers \n",
    "def testFuncNew(text= 'This is sample text.',sw=rsw,key = 'e'):\n",
    "    if key == 'e':\n",
    "        text = text.lower()\n",
    "        text = ' '.join([word for word in text.split() if word not in sw])\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        text = ' '.join([word for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "#loop over the five urls\n",
    "for firm in firms:\n",
    "    if len(okurl[firm]) == 0:\n",
    "        Header_policies.append(['__NO__ __OK__ __URL__'])\n",
    "        Paragraph_policies.append(['__NO__ __OK__ __URL__'])\n",
    "        print('%s __NO__ __OK__ __URL__' %firm)\n",
    "        Headers_Lengths.append(0)\n",
    "\n",
    "    for index in okurl[firm]:\n",
    "        policies_h = []\n",
    "        policies_p = []\n",
    "        try:                \n",
    "            if urls[firm][index].endswith('.pdf'):\n",
    "                flag = 'Document is PDF'\n",
    "                policies_h.append('Document is PDF')\n",
    "                policies_p.append('Document is PDF')\n",
    "                Headers_Lengths.append(0)\n",
    "            else:\n",
    "                sauce = urllib.request.urlopen(Request(urls[firm][index], headers={'User-Agent': 'Mozilla/5.0'})).read()\n",
    "                #read the html files using bs - use 'lxml' is 'html5lib' doesn't work\n",
    "                soup = bs.BeautifulSoup(sauce,'html5lib')\n",
    "                headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5','h6'])\n",
    "                paras = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5','h6','p','ul','table'])\n",
    "                headers = [header.get_text() for header in headers]\n",
    "                paras = [para.get_text() for para in paras]\n",
    "                \n",
    "                headers = [x for x in headers if x != '']\n",
    "                #extract headers - isolate bold texts from the main text\n",
    "                for header_index,header_word in enumerate(headers):\n",
    "                    headers[header_index] = testFuncNew(text = header_word,key='ne').upper()\n",
    "                policies_h = headers\n",
    "                for para_index,para_sentence in enumerate(paras):\n",
    "                    paras[para_index] = testFuncNew(text = para_sentence,key='ne').lower()\n",
    "                policies_p = paras\n",
    "                flag = \"Downloaded %d headers\" % len(headers)\n",
    "                Headers_Lengths.append(len(headers))\n",
    "            \n",
    "        except Exception as e:\n",
    "            flag = \"Error: \" + str(e)\n",
    "            policies_h.append(flag)\n",
    "            policies_p.append(flag)\n",
    "            Headers_Lengths.append(0)\n",
    "        finally:\n",
    "            print('Firm: %s, URL %s, %s' % (firm,str(index),flag))\n",
    "            Header_policies.append(policies_h)\n",
    "            Paragraph_policies.append(policies_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eleminating blank headers \n",
    "for index,header in enumerate(Header_policies):\n",
    "    temp = [x for x in header if x != '']\n",
    "    Header_policies[index] = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Header_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "headers_paragraphs = []\n",
    "Firm_Info = []\n",
    "\n",
    "\n",
    "for firm,firm_name in list(enumerate(firms)):\n",
    "    alltext = Paragraph_policies[firm]\n",
    "    headers = Header_policies[firm]\n",
    "    temp1 = []\n",
    "    \n",
    "    n_headers = len(headers)\n",
    "    n_alltext = len(alltext)\n",
    "    \n",
    "    if n_headers == 1:\n",
    "        paragraphs.append(' '.join(str(e) for e in alltext))\n",
    "        temp1.append(' '.join(str(e) for e in alltext))\n",
    "        Firm_Info.append(firm_name)\n",
    "        \n",
    "    else:\n",
    "        for head in list(range(1,n_headers)):\n",
    "            check1 = headers[head-1]\n",
    "            check2 = headers[head]\n",
    "            pos1 = alltext.index(check1.lower())\n",
    "            pos2 = alltext.index(check2.lower())\n",
    "            if (pos2 - pos1) > 1:\n",
    "                paragraphs.append(' '.join(str(e) for e in alltext[(pos1+1):(pos2)]))\n",
    "                temp1.append(' '.join(str(e) for e in alltext[(pos1+1):(pos2)]))\n",
    "                Firm_Info.append(firm_name)\n",
    "            else:\n",
    "                paragraphs.append(\"__No_ __text__ __in__ __the__ __header__\")\n",
    "                temp1.append(\"__No_ __text__ __in__ __the__ __header__\")\n",
    "                Firm_Info.append(firm_name)\n",
    "            if head==(n_headers-1):\n",
    "                paragraphs.append(' '.join(str(e) for e in alltext[(pos2+1):]))\n",
    "                temp1.append(' '.join(str(e) for e in alltext[(pos2+1):]))\n",
    "                Firm_Info.append(firm_name)\n",
    "\n",
    "    for head,para in enumerate(temp1):\n",
    "        temp = [headers[head],para]\n",
    "        headers_paragraphs.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export headers to csv file - to be saved in the working library\n",
    "df_headers = pd.DataFrame(Header_policies)\n",
    "df_headers.insert(loc=0, column='firm', value=firms)\n",
    "df_headers.to_csv(\"headers.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract headers and the corresponding paragraphs\n",
    "headers_paragraphs_df = pd.DataFrame(headers_paragraphs,columns=(\"Headers\",\"Paragraphs\"))\n",
    "headers_paragraphs_df['Firm_Info'] = Firm_Info\n",
    "headers_paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Paragrraphs with only 100 or more characters.\n",
    "len_list = [len(x) for x in headers_paragraphs_df['Paragraphs']]\n",
    "out_index = [i for i,x in enumerate(len_list) if x<100]\n",
    "headers_paragraphs_df.drop(headers_paragraphs_df.index[out_index],inplace=True)\n",
    "headers_paragraphs_df = headers_paragraphs_df.reset_index(drop=True)\n",
    "headers_paragraphs_df.to_csv(\"headers_paragraphs.csv\")\n",
    "headers_paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will generate a csv file for output firms.\n",
    "res = list(set(headers_paragraphs_df['Firm_Info'].tolist()))\n",
    "with open('Output_firm.csv', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in res:\n",
    "        writer.writerow([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peparing data for creating text file.\n",
    "File_Headers = headers_paragraphs_df['Headers'].tolist()\n",
    "File_Paragraphs = headers_paragraphs_df['Paragraphs'].tolist()\n",
    "ls = headers_paragraphs_df['Firm_Info'].tolist()\n",
    "File_Firms = [x for i, x in enumerate(ls) if ls.index(x) == i]\n",
    "temp1 = []\n",
    "for firm in File_Firms:\n",
    "    temp = []\n",
    "    for item in range(0,len(File_Headers)):\n",
    "            if firm in ls[item]:\n",
    "                temp.append(item)\n",
    "    temp1.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text file to be saved in the working directory\n",
    "thefile = open('test.txt', 'w')\n",
    "\n",
    "for index,firm in enumerate(File_Firms):\n",
    "    thefile.write(\"\\n# %s \\n\" %firm)\n",
    "    #thefile.write(\"\\n--@@@@%s@@@@--\\n\" %urls[firm][0])\n",
    "    for i in temp1[index]:\n",
    "        try:\n",
    "            thefile.write(\"\\n## %s \\n %s \\n\" % (File_Headers[i],File_Paragraphs[i]))\n",
    "            print(\"Header: %s Success\"%(File_Headers[i]))\n",
    "        except Exception as e:\n",
    "           \n",
    "                print(e)\n",
    "        \n",
    "thefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(okurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function(tdm_df) would be used to extract words from the policies - eventually to create TDM matrix\n",
    "from pandas import *\n",
    "def tdm_df(doclist, stopwords = [], remove_punctuation = True, \n",
    "           remove_digits = True, sparse_df = False):\n",
    "    \n",
    "    # Create the TDM from the list of documents.\n",
    "    tdm = txtm.TermDocumentMatrix()\n",
    "  \n",
    "    for doc in doclist:\n",
    "        if remove_punctuation == True:\n",
    "            doc = doc.translate(None, string.punctuation.translate(None, '\"'))\n",
    "        if remove_digits == True:\n",
    "            doc = doc.translate(None, string.digits)\n",
    "            \n",
    "        tdm.add_doc(doc)\n",
    "    \n",
    "    # Push the TDM data to a list of lists,\n",
    "    # then make that an ndarray, which then\n",
    "    # becomes a DataFrame.\n",
    "    tdm_rows = []\n",
    "    for row in tdm.rows(cutoff = 1):\n",
    "        tdm_rows.append(row)\n",
    "        \n",
    "    tdm_array = np.array(tdm_rows[1:])\n",
    "    tdm_terms = tdm_rows[0]\n",
    "    df = DataFrame(tdm_array, columns = tdm_terms)\n",
    "    \n",
    "    # Remove stopwords from the dataset, manually.\n",
    "    # TermDocumentMatrix does not do this for us.\n",
    "    if len(stopwords) > 0:\n",
    "        for col in df:\n",
    "            if col in stopwords:\n",
    "                del df[col]\n",
    "    \n",
    "    if sparse_df == True:\n",
    "        df.to_sparse(fill_value = 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary to store data frames for each url \n",
    "\n",
    "TDM = {}\n",
    "for i in firms:\n",
    "    TDM[i] = {}\n",
    "    \n",
    "TDM.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract unigrams for each firm \n",
    "#policy that is longest and meets the mimimum criteria is used for each firm\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_rows = None\n",
    " \n",
    "for firm in firms:\n",
    "    for index in okurl[firm]:\n",
    "        TDM[firm][index] = {}\n",
    "        TDM[firm][index] = tdm_df([scrape_policies[firm][index]],stopwords = rsw,remove_punctuation = False,remove_digits = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store unique firms\n",
    "master_data=[]\n",
    "selected_firms=[]\n",
    "\n",
    "for firm in firms:\n",
    "    index = okurl[firm]\n",
    "    if len(index) != 0:\n",
    "        master_data.append(scrape_policies[firm][okurl[firm][0]])\n",
    "    #else:\n",
    "      #  master_data.append('__aemptya__')\n",
    "    \n",
    "len(master_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create term document matrix (TDM) using the master data\n",
    "\n",
    "master_tdm = tdm_df(master_data,stopwords = rsw,remove_punctuation = False,remove_digits = False)\n",
    "master_tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geographic indicator indicates if name(s) of any of the US state(s)\n",
    "#appear in the document term matrix (TDM) for a given firm \n",
    "geography_information = []\n",
    "geography_indicator = []\n",
    "\n",
    "#names of the states have been appended - can be seen in the right most column of the output file\n",
    "for index,firm_data in enumerate(master_data):\n",
    "    temp = \"\"\n",
    "    for state in states:\n",
    "        if state in firm_data:\n",
    "            temp = temp + ' ' + state\n",
    "    if len(temp) != 0:\n",
    "        geography_information.append(temp)\n",
    "        geography_indicator.append(1)\n",
    "    else:\n",
    "        geography_information.append('__NO__')\n",
    "        geography_indicator.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign geographic information to the TDM\n",
    "master_tdm[\"geography_indicator\"] = geography_indicator\n",
    "master_tdm['geography_information'] = geography_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export unigram matrix to csv\n",
    "master_tdm.to_csv(\"unigram.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will prepare data to create bigrams \n",
    "#the structure of the bigram file (BDM) will remain same as that of the TDM\n",
    "def testFuncNew(text= \"Hello, I am fine\",rsw=rsw):\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in rsw])\n",
    "    return text\n",
    "#store bigrams\n",
    "list_data = []\n",
    "\n",
    "#loop over each firm\n",
    "for firm in firms:\n",
    "    length = okurl[firm]\n",
    "    breaks = firm.lower().split()\n",
    "    for i in breaks:\n",
    "        rsw.append(i)\n",
    "    if len(length) != 0:\n",
    "        tdm = txtm.TermDocumentMatrix()\n",
    "        doc = scrape_policies[firm][okurl[firm][0]]\n",
    "        tdm.add_doc(doc)\n",
    "\n",
    "        tdm_rows = []\n",
    "    \n",
    "        for row in tdm.rows(cutoff = 1):\n",
    "            tdm_rows.append(row)\n",
    "        \n",
    "        tdm_array = np.array(tdm_rows[1:])\n",
    "        tdm_terms = tdm_rows[0]\n",
    "\n",
    "        text = \"\"\n",
    "        for index , word in enumerate(tdm_terms):\n",
    "            text = text + \" \" + word \n",
    "        text = testFuncNew(text,rsw = rsw)    \n",
    "        list_data.append(text)\n",
    "    else:\n",
    "        list_data.append(\"\")\n",
    "list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk to create consin dissimilarity matrix\n",
    "#Function for eleminating stopwords and \n",
    "\n",
    "counter_sum = Counter()\n",
    "for line in list_data:\n",
    "    bigrams = list(nltk.bigrams(line.split()))\n",
    "    bigramsC = Counter(bigrams)\n",
    "    counter_sum += bigramsC\n",
    "#export the bigram matrix to a csv file - to be saved in the working directory\n",
    "\n",
    "with open('bigram.csv', 'w', newline='') as csvfile:\n",
    "    header = sorted(counter_sum, key=lambda x: str(type(x)))\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for line in list_data:\n",
    "        bigrams = list(nltk.bigrams(line.split()))\n",
    "        bigramsC = Counter(bigrams)\n",
    "        cs = dict(counter_sum)\n",
    "        bc = dict(bigramsC)\n",
    "        row = {}\n",
    "        for element in list(cs):\n",
    "            if element in list(bc):\n",
    "                row[element] = bc[element]\n",
    "            else:\n",
    "                row[element] = 0\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute cosine similarities \n",
    "#values are stored in a matrix - looks similar to a var-covar or correlation matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=1)\n",
    "text = vect.fit_transform(list_data)\n",
    "\n",
    "temp_array = (text * text.T).A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export cosine similarities matrix to a csv file\n",
    "import pandas as pd\n",
    "# \n",
    "df_cosine = pd.DataFrame(temp_array,columns=firms,index=firms)\n",
    "df_cosine\n",
    "df_cosine.to_csv(\"F:/Cosine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a worldcloud for the unigrams.\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate word cloud image\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "\n",
    "text = str(master_tdm)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to json # if does not work run the very first tab again(import the libraries again) and then run \n",
    "path = 'C:/Users/Lenovo/Desktop/NLP/'\n",
    "with open(path + 'hmda_scraped_policies.json', 'w') as fp:\n",
    "    json.dump(scrape_policies, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
